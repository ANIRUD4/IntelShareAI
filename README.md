# IntelShare ğŸ§   
**Human-in-the-Loop Personalized AI on the Edge**

IntelShare is a system that allows users to **teach AI systems using natural interaction** â€” vision, voice, and confirmation â€” without datasets, retraining, or machine learning expertise.

The system learns incrementally from real-world inputs and always keeps the **human in control**.

---

## ğŸš€ Key Features

- ğŸ“· Vision-based learning using live camera input  
- ğŸ™ï¸ Voice-based labeling using **offline speech recognition (Vosk)**  
- ğŸ” Human-in-the-loop confirmation and correction  
- ğŸ“ˆ Incremental learning without retraining models  
- ğŸ§  Embedding-based knowledge storage  
- âš™ï¸ Edge-friendly and hardware-ready architecture  

---

## ğŸ§© High-Level Workflow

Camera â†’ Embedding â†’ Learn â†’ Store Knowledge
â†“
Inference
â†“
Human Confirmation
â†“
Incremental Update


The system never makes autonomous decisions â€” every prediction is verified or corrected by the user.

---

## ğŸ› ï¸ Tech Stack

### Backend
- Python
- FastAPI
- Vosk (offline speech recognition)
- OpenCV
- NumPy

### Frontend
- HTML
- JavaScript
- Browser Camera & Microphone APIs

### AI / ML
- Embedding-based similarity
- Confidence-based incremental updates
- No datasets, no retraining

---

## ğŸ“ Project Structure


The system never makes autonomous decisions â€” every prediction is verified or corrected by the user.

---

## ğŸ› ï¸ Tech Stack

### Backend
- Python
- FastAPI
- Vosk (offline speech recognition)
- OpenCV
- NumPy

### Frontend
- HTML
- JavaScript
- Browser Camera & Microphone APIs

### AI / ML
- Embedding-based similarity
- Confidence-based incremental updates
- No datasets, no retraining

---

## ğŸ“ Project Structure

intelshare/
â”œâ”€â”€ backend/
â”‚ â”œâ”€â”€ routes/ # learn, infer, confirm, speech
â”‚ â”œâ”€â”€ orchestrator.py
â”‚ â””â”€â”€ main.py
â”œâ”€â”€ perception/
â”‚ â””â”€â”€ embedding_engine.py
â”œâ”€â”€ interaction/
â”‚ â”œâ”€â”€ voice_listener.py
â”‚ â””â”€â”€ voice_api.py
â”œâ”€â”€ data/
â”‚ â””â”€â”€ knowledge_units/
â”œâ”€â”€ frontend/
â”‚ â”œâ”€â”€ index.html
â”‚ â””â”€â”€ script.js
â””â”€â”€ README.md


---

## âš™ï¸ Installation & Setup

### 1ï¸âƒ£ Clone the Repository
git clone <repo-url>
cd intelshare


2ï¸âƒ£ Install Python Dependencies
pip install -r requirements.txt

3ï¸âƒ£ Install FFmpeg (Required for Speech)

FFmpeg is required to convert browser-recorded audio into a format usable by Vosk.

Download FFmpeg:
https://www.gyan.dev/ffmpeg/builds/

Add the bin directory to your system PATH

Verify installation:

ffmpeg -version

4ï¸âƒ£ Download Vosk Model

Download an English Vosk model:
https://alphacephei.com/vosk/models/

Example:
vosk-model-small-en-us-0.15


Place it inside:
models/

5ï¸âƒ£ Run the Backend

uvicorn backend.main:app --reload


6ï¸âƒ£ Run the Frontend

Open frontend/index.html using Live Server or any local web server
